---
title: "概率图模型——R语言  <br />  第3章 学习参数"
author: ' **教师：杨 伟**  <br />   **电话：18910162189**  <br />  **email: yangyxq@ruc.edu.cn** '
date: '**`r Sys.Date()`**'
output:
  slidy_presentation:
    css: markdown.css.txt
    incremental: yes
  beamer_presentation:
    incremental: yes
  powerpoint_presentation:
  ioslides_presentation:
    incremental: yes
    widescreen: yes
---

# 第3章 学习参数

- 构建概率图模型的3个步骤：定义随机变量（即图中节点）、定义图的结构（或结构学习）、定义每个局部分布（即节点条件概率）的数值参数

- 人工设定或指定每个局部概率分布的数值有困难，可获取到大量真实数据并使用**参数学习的方法（参数拟合或模型校准）**估计参数取值

- 参数学习是机器学习中的重要课题，如何使用数据集为给定的图模型学习参数：完全可观测到的数据；部分可观测到的数据（需要更多先进技术）

  * + __概率查询__：观察到随机变量子集$E$，取值为$e$（即一个实例），称之为证据；可计算另一个随机变量子集$Y$的后验概率分布$P(Y|E=e)$
  
    + __最大后验概率(MAP)查询__：也称为**最可能解释(MPE)查询**，即给定证据$E=e$的条件下，对变量子集$Z$找出拥有最大概率的联合赋值；MAP联合赋值定义为$$MAP(Z|E=e)=argmax_zP(z,e)$$
  
- 本章目的：介绍解决精确推断问题即上述查询问题的主流算法；而推断问题可化解为通过贝叶斯规则找出后验概率的问题

- 设定$\chi$为模型中所有随机变量的集合，$E$是被观察到的变量子集（证据），$Z$是隐含变量或未被观察到的变量子集，那么计算概率图模型的推断如下：$$P(Z|E,\theta)=\frac{P(Z,E|\theta)}{P(E|\theta)}=\frac{P(Z,E|\theta)}{\sum_{z \in Z}P(Z=z,E|\theta)}$$

- 例如，医学问题中，给定观察到的症状集合，推断所有可能的疾病；语音识别系统中，推断被记录的声音（即说话者的语音）中最可能的单词序列；雷达跟踪系统中，从雷达读数中推断跟踪物体位置的概率分布；推荐系统中，给定售卖网站上用户最近的点击数据后，推断待售产品的后验概率分布，以便给客户提供最优的$5$个产品的排序和推荐

- 所有这些问题以及更多复杂问题都需要计算后验概率分布，介绍以概率图模型(PGMs)为基础算法和高效算法的新方法：构建图模型、变量消解算法、和积与信念更新算法、联结树算法、概率图模型示例，目前都可在R程序包中实现，例如**gRain**、**gR**、**rHugin**和**bnlearn**程序包中的函数

# 2.1 构建图模型 

- 图模型的设计考虑到两个不同的方面。一是确定变量，二是确定图（首先是图结构）

- 确定图模型中涉及的变量：可以观察到或度量到的事实，也表示一个或真或假（或是或否）的简单事实

  * + 变量可捕捉与问题相关的局部内容，而却无法直接度量或估计这些与问题相关的变量；例如，外科医生可度量病人的一系列症状，但疾病并不是可直接观察到的事实，且只能通过几个症状的观察结果推断出来

    + 直接推出病人得了感冒不太符合事实，除非医生根据鼻病毒增殖情况推断患者$\xrightarrow[鼻病毒数量]{鼻黏液采样}$上呼吸道（鼻子）病毒感染；或者，从简单的症状例如头痛、流鼻涕等推断结论；此时，严格说**病人感冒变量**不是被直接观察到的

- 确定图模型中的图结构：不同变量之间的直接或间接的依赖（关联或交互）关系，即统计上的相关性，但在图模型中，这种依赖理解更加宽泛

# 2.1.1 随机变量的类型

- 许多科学领域使用离散型变量进行建模非常常见，其构建的图模型背后的数学逻辑易于理解和实现

- 设一个离散随机变量$X$定义在有限样本空间$S={\nu_1, \nu_2, \cdots, \nu_n}$上

  * + 一个骰子定义的样本空间为$S=\left\{1, 2, 3, 4, 5, 6\right\}$
  
    + 一枚硬币定义的样本空间为$S=\left\{H, T\right\}$
  
    + 一个症状定义的样本空间为$S=\left\{true, false\right\}$
  
    + 单词中的一个字母定义的样本空间为字母集合$S=\left\{a, b, c, d, e, \cdots, z\right\}$
  
    + 大量英文单词中一个单词的样本空间为单词集合$S=\left\{the, at, in, bread, \cdots, computer, \cdots \right\}$，这个集合会很大，但也是有限的
  
    + 字号尺寸大小定义的样本空间为数值集合$S=\left\{0, 1, 2, 3, \cdots, 1000\right\}$pt
  
- 设一个连续随机变量是定义在一个连续的样本空间$\mathbb{R}$，$\mathbb{C}$或任意区间或多维空间$\mathbb{R}^n$上，有时把维度分成$n$个不同的定义在$\mathbb{R}$上的随机变量也是有意义的

  * + 距离公里数、温度、价格、随机变量的平均值或方差等
  
    + 考虑贝叶斯方法时，所有的数值都可看作随机变量的取值，因此如果定义一个服从正态分布$N(\mu, \sigma^2)$的随机变量，也可进一步把$\mu$，$\sigma^2$理解为随机变量；图模型中把许多参数当成随机变量并在图中连接起来通常是很有用的

# 2.1.2 构建图结构

- 图结构即变量之间的连接是基于常识、因果交互或其它存在于变量间足够强的依赖关系而获得的，当然也可通过算法自动从数据集中学习这些连接（文献中的关键词：因果关系(causality)、稀疏模型(sparse models)、因式分解(factorization)）

- 当两个变量连接时，模型和信息流会发生什么变化？重要概念：$d$-分离

  * + 如果$X$ 和$Y$ 之间的所有通路都被$Z$ 阻塞，那么我们就说$Z$ 有向分隔(directed separate)$X$ 和$Y$，简称$d$-分隔(d-separate)$X$ 和$Y$
  
      ![图2-0 $X$ 和$Y$ 之间的通路被$Z$ 阻塞的3种情况](./图2-0.jpg "图2-0")

- 另一种生成图模型的方法是模块化，即通过简单的模块构建复杂的图模型且可通过扩展图结构来扩展现有模型

- **参数学习和查询问题都归结为学习和推断算法的应用**

# 2.1.2 构建图结构
## 概率专家系统

- 肺结核医疗诊断：肺结核是由结核杆菌引起的，只有临床生物分析检验可检测出这种病毒且确认是否为肺结核；然而，物理检验也可揭示一些肺结核的线索，协助外科医生推断是否需要全面的临床检验以得出病人体内是否存在病毒的判断；完整的肺结核医疗评估必须包含医疗历史、物理检验、胸部透视以及微生物检验<http://en.wikipedia.org/wiki/Tuberculosis_diagnosis>

- **确定模型（系统）中用到的随机变量**：

  * + $Cough(C)$：超过$3$周的咳嗽，取值**是或否**；
  
    + $Chest Pain(P)$：胸脯疼痛，取值**是或否**；
  
    + $Hemoptysis(H)$：咯血，取值**是或否**；
  
    + $Night Sweats(N)$：盗汗，取值**是或否**；
  
    + $Loss of appetite(L)$：食欲不振，取值$3$个等级$\left\{low, medium, strong\right\}$，中医症状比较主观，表示饭量减少的程度
  
    + 由于只有微生物研究(microbiological study)可能确定是否肺结核，而其它的症状或变量只能反向推断肺结核的存在，故还需定义两个随机变量：是否发现病毒$M$（两个概率值）；是否患有肺结核或是否肺结核呈阴性$tuberculosis(T)$（四个概率值）

- **确定图结构并学习参数**：连接随机变量的图形，估计每个变量相互连接的先验概率或者图中每个节点的参数
  
  * + 从二元症状$C, P, H, N$和$L$开始，它们都由疾病$T$引起的，如下**图2-1**：
    
      ![](./图2-1.jpg "图2-1")
    + 在上面的图模型中处理原因和结果的模式是很常见的，可把同样的思路用到微生物研究$M$和疾病$T$的关系中，如**图2-2**所示：
    
      ![](./图2-2.jpg "图2-2")
      
    + 把两个子模型的图结构即**图2-1**和**图2-2**组合成更加复杂的图模型，其图结构如**图2-3**：

      ![](./图2-3.jpg "图2-3")
      
    + 事实上，可以在同一个图中添加更多症状和疾病，以便可以区分诸如肺结核和肺炎以及其它包含类似症状的疾病；通过计算给定症状下每个疾病的后验概率，辅助医生判断采取什么医疗方案来应对最可能的疾病；这种形式的概率图模型有时也称为**概率专家系统**

# 2.1.2 构建图结构
## 概率图模型的基本结构

- 同一个事实有很多原因的图结构，如下**图2-4**：假设原因$C_1,C_2,\cdots,C_n$都是二元变量，以及事实$F$也是二元变量，对应的概率分布是$P(F|C_1,C_2,\cdots,C_n)$，它可用一个带有$2^{n+1}$个值的表确定；如果$n=10$，这个值并不大，只需要`r 2^(10+1)`个值，但如果$n=20$，则需要`r 2^(20+1)`个值，占用了很大内存；如果变量取值$k$，那就需要$k^(n+1)$个值，那就更加庞大了

  ![](./图2-4.jpg "图2-4")

  * + 其实有些原因并没有直接与事实关联，而是导致其它原因的原因，在这种情况下，可考虑给出原因的层级图结构，如下**图2-5**中处理了**8**个原因，但每个局部条件概率，例如$P(D_1|C_1,C_2,C_3)$最多只涉及**4**个变量，就变得容易处理了
    
      ![](./图2-5.jpg "图2-5")

- 考虑时间上变量序列的图结构：这种结构不会捕捉因果关系而是捕捉变量在时间上的顺序，假设随机变量$X_t$表示模型$X$在时间$t$的状态，并假设模型$X$的当前状态可以预测下一刻的状态，因此，给定前一时刻状态$X_{t-1}$时，模型$X$当前状态的概率分布为$P(X_t|X_{t-1})$，其中$t$和$t-1$表示时间；另外，再假设另一个随机变量$O_t$在时间$t$的观察结果直接依赖于$X_t$的状态（但不属于模型$X$），则确定$P(O_t|X_t)$是合理的，如下**图2-6**

  ![](./图2-6.jpg "图2-6")
  
  * + 当随机变量$X_t$和$O_t$是离散型的，则此概率图模型也叫作**隐马尔可夫模型(Hidden Markov Model)**或**马尔可夫模型(Markov Model)**；它的当前状态只依赖于之前的状态，即$X_t$只依赖于$X_{t-1}$
    
    + 当随机变量$X_t$和$O_t$服从正态分布（高斯分布），则此概率图模型就是著名的**卡尔曼滤波器(Kalman filter)**，一种最优化自回归数据处理算法
    
- 结合了两个隐马尔可夫模型的图结构：其中一个模型$Y$也是另一个模型$X$的原因，如下**图2-7**

  ![](./图2-7.jpg "图2-7")

  * + 这个概率图模型的联合概率分布可分解成$$\begin{matrix}
  \\ P(\chi = \left\{X,O,Y,W\right\})=P(Y_{t-2})\cdot P(W_{t-2}|Y_{t-2})\cdot P(Y_{t-1}|Y_{t-2})\cdot P(W_{t-1}|Y_{t-1})\cdot P(Y_t|Y_{t-1})\cdot P(W_t|Y_t)
  \\ P(X_{t-2}|Y_{t-2})\cdot P(O_{t-2}|X_{t-2})\cdot P(X_{t-1}|Y_{t-1},X_{t-2})\cdot P(O_{t-1}|X_{t-1})\cdot P(X_t|Y_t,X_{t-1})\cdot P(O_{t-1}|X_{t-1})
  \end{matrix}$$
    
# 2.2 变量消除法

- 正如前面的模型示例所见，当人们处理推断问题时，需要面对一个NP-难题即导致参数估计算法有指数级的时间复杂度

- 推断是指给定模型中变量子集的观察值后，计算其它变量子集的后验概率；这两个子集通常可选取任一不相交的子集

- 设$\chi$为图模型中所有变量的集合，$Y,E\subset \chi$是两个不相交的变量子集；$Y$为查询子集，$E$为观察到的证据（观测子集，有具体取值），需计算后验概率$P(Y|E=e)$，根据概率推理中的贝叶斯理论，得到查询一般形式是$P(Y|E=e)=\frac {P(Y,e)} {P(e)}$，把$P(Y,e)$看作$Y$上的函数，使得$P(Y,e) \rightarrow P(y,e)=P(Y=y,E=e)$——即同时取值$Y=y,E=e$的概率

- 定义$W=X-Y-E$，即图模型中既不是查询变量又不是观测变量的变量子集，可以计算$P(y,e)=\sum_{w \in W} P(y,e,w)$，即沿着$W$进行边缘化

- 同样推理，可计算证据$P(E=e)$的概率，即$P(e)=\sum_y P(Y,e)$

- 贝叶斯推理的一般机制：沿着不需要的和观测到的变量进行边缘化，只剩下要查询的变量

- **图2-8**显示一个简单示例

  ![图2-8](./图2-8.jpg "图2-8")
  
  * + 图模型编码的概率分布：$\begin{matrix}
  P(ABCD)=P(A) \cdot P(B|A) \cdot P(C|AB) \cdot P(D|ABC)=P(A) \cdot P(B|A) \cdot P(C|B) \cdot P(D|C)
  \end{matrix}$，一个非常简单的推理链，可以用来展示变量消解算法
  
    + 图中每个节点都关联了一个潜在的函数，即给定父节点条件下，$P(A)$、$P(B|A)$、$P(C|B)$、$P(D|C)$的条件概率
  
    + 如果已知$P(A)$的关联函数，就需要通过沿$A$边缘化计算出$P(B)=\sum_a P(B|a)P(a)$
  
    + 为了理解这个推断，详细地写出求和公式：如果$A \in \mathbb{R}^k$，$B \in \mathbb{R}^m$（即$A$有$k$个可能的取值，$B$有$m$个可能的取值），则$$\begin{matrix}
    P(B=i)=\sum_a P(a)P(B=i|a)
    \\    =P(A=1)P(B=i|A=1)+
    \\     P(A=2)P(B=i|A=2)+
    \\     \cdot \cdot \cdot
    \\     P(A=k)P(B=i|A=k)
    \end{matrix}$$
  
    + 这个公式需要计算$B$的每一个$m$值，由此可边缘化$A$，可以得到一个等价的图模型如**图2-9**所示
  
      ![图2-9](./图2-9.jpg "图2-9")
    + $B$的分布已经通过$A$的信息进行了更新，使用相同的算法，可获取$C$和$D$的边缘分布$P(C)$和$P(D)$
  
    + 最终获取$P(D)$边缘化的完整求和形式：$$P(D)=\sum_c \sum_b \sum_a P(A) \cdot P(B|A) \cdot P(C|B) \cdot P(D|C)$$
    
      - 因为每次求和只需关注特定的变量，故可重写求和公式：$$P(D)=\sum_c P(D|C) \sum_b P(C|B) \sum_a P(A)P(B|A)$$
      
      - 这极大地简化了计算量：对于给定的PGMs，$\mathbb{R}^k$中$n$个变量的联合概率分布的计算复杂度只有$O(k^n)$（不考虑PGMs中条件独立性，原始贝叶斯公式下的计算复杂度为$k \cdot k^2 \cdots k^n $）
      
      - 变量消除法的主要思想：对变量求和，并在下一步中重用之前的结果。得益于图模型的结构，每一步求和只依赖少数变量，可以把结果沿着图中的路径储存起来
      
# 2.3 和积与信念更新

- 变量消除法核心在于边缘化，即通过在一个变量（或者变量子集）上求和来把变量从表达式中消解出来

- 定义$\varphi$为联合概率分布分解中的一个因子，使用如下属性来泛化和优化变量消除算法：

  * + 对称律：$\varphi_1 \varphi_2 = \varphi_2 \varphi_1$
  
    + 结合律：$\left (\varphi_1 \cdot \varphi_2\right ) \cdot \varphi_3 = \varphi_1 \cdot \left (\varphi_2 \cdot \varphi_3\right )$
    
    + 如果$X \notin \varphi_1$：$\sum_X \left (\varphi_1 \cdot \varphi_2\right )=\varphi_1 \sum_X \varphi_2$
    
- 回到之前章节中的联合分布$P(ABCD)$，$$\begin{matrix}
\\ P(D)=\sum_C \sum_B \sum_A \varphi_A \varphi_B \varphi_C \varphi_D 
\\     =\sum_C \sum_B \varphi_C \varphi_D \left (\sum_A \varphi_A \varphi_B \right )
\\     =\sum_C \varphi_D \left (\sum_B \varphi_C \left (\sum_A \varphi_A \varphi_B \right ) \right )
\end{matrix}$$

  * + 给定PGMs，有$P(ABCD)=P(A) \cdot P(B|A) \cdot P(C|B) \cdot P(D|C)$，则因子$$\begin{matrix}
  & \\ \varphi_A=P(A)
  & \varphi_B=P(B|A)
  & \\ \varphi_C=P(C|B)
  & \varphi_D=P(D|C)
  \end{matrix}$$
  
    + 最后，反复出现的主要表达式是在一个因子上的和积结果，可写作$\sum_Z \prod_{\varphi \in \Phi} \varphi$。上例中$Z=\left \{A,B,C \right \}$，$\Phi=\left \{\varphi_A,\varphi_B,\varphi_C, \varphi_D \right \}$
    
    + 因此，通常如果可找到有向图模型中因子或变量的优质顺序，就可用和积公式逐步消除每个变量直到得到想要的子集
    
    + 消除顺序必须可以边缘化每个包含待消除变量的因子，生成可再次使用的新因子
    
# 2.3 和积与信念更新
## 和积变量消除算法

- 采用《概率图模型Probabilistic Graphical Models, D. Koller, and N. Friedman, 2009, MIT Press》中叫作和积变量消除算法(sum-product variable elimination algorithm)：

  * + $\Phi$：因子集合
  
    + $Z$：要消除的变量集合
    
    + $\prec$：$Z$上的序
    
      1. 设$Z_1,\ldots,Z_k$是$Z$上的序，满足$Z_i \prec Z_j$当且仅当$i<j$
        
      2. 循环$i=1,\ldots,k$（比如$Z_1=A,Z_2=B,Z_3=C$）
        
      3. $\Phi=SumProductEliminateVar \left (\Phi,Z_i \right )$
        
      4. $\varphi^* = \prod_{\varphi \in \Phi} \varphi$
        
      5. 返回$\varphi^*$
      
    + 其中第3步中的函数算法执行如下：当收到消除变量或因子的顺序后，对每个变量（或因子）使用算法消除变量并使用这个函数的结果缩小因子集合，然后乘以剩下的因子并返回结果
    
    + 此和积消除算法的第3行子过程如下，目的是一次消除一个变量（或因子）：
    
      - 1）${\Phi}' = \varphi \in \Phi$：$Z_i \in Scope \left (\varphi \right )$
      
      - 2）${\Phi}'' = \Phi - {\Phi}'$
      
      - 3）$\Psi = \prod_{\varphi \in {\Phi}'} \varphi$
      
      - 4）$\tau = \sum_{Z_i} \Psi$
      
      - 5）返回${\Phi}'' \cup \left \{\tau \right \}$
      
    + 此子过程的思想：首先乘上变量$Z$出现时的潜在函数，然后边缘化（第4行）消除变量$Z$；最后，算法返回因子集合，这个集合已经去掉所有包含$Z$的因子（第2行）；新的和积因子通过对$Z$的边缘化得到，并添加进行（第5行）；注意，第1行选取了包含所有待消除的变量$Z$的因子
    
# 2.3 和积与信念更新
## 在一个新示例上用R执行和积消除算法 

- 如**图2-10**所示

  ![图2-10](./图2-10.jpg "图2-10")
  * + 它联合概率分布的分解形式：$P(ABCD)=P(A) \cdot P(B|A) \cdot P(C|B) \cdot P(D|B)$
  
    + 条件概率分布由如下矩阵定义：
    
      ```{r Sum-product algorithm, echo=TRUE, tidy=TRUE, warning=FALSE, message=FALSE, comment=""}
      # Sum-product and beliefs update examples
      A=matrix(c(.8,.2),2,1)
      B=matrix(c(.6,.4,.3,.7),2,2)
      C=matrix(c(.5,.5,.8,.2),2,2)
      D=matrix(c(.3,.7,.4,.6),2,2)
      ```
     
    + 条件概率分布用矩阵中的列表示，例如$B$的条件概率是：
    
      ```{r}
      B
      ```
      
    + 即$\begin{matrix}
    & \\ P(B=1|A=1)=0.6 & P(B=1|A=0)=0.3
    & \\ P(B=0|A=1)=0.4 & P(B=0|A=0)=0.7
    \end{matrix}$
    
    + 要消除的变量集合是${A,B,C}$，最终获得$D$的边缘概率分布，算法实现如下：
    
      1. 首先消除顺序中的$A$，获得$P(B,C,D)$，因此需要边缘化$A$：$$P(B)=\sum_A P(A) \cdot P(B|A) \\
      = \left\{\begin{matrix}
        P(B=1)=\sum_A P(A) \cdot P(B=1|A) = P(A=1)P(B=1|A=1)+P(A=0)P(B=1|A=0) \\
        P(B=0)=\sum_A P(A) \cdot P(B=0|A) = P(A=1)P(B=0|A=1)+P(A=0)P(B=0|A=0)
        \end{matrix}\right. \\
        = \left\{\begin{matrix}
        P(B=1)=0.8 \times 0.6 + 0.2 \times 0.3=0.48+0.06 \\
        P(B=0)=0.8 \times 0.4 + 0.2 \times 0.7=0.32+0.14
        \end{matrix}\right.$$
        
         - 用向量形式计算为：
         $$A^T \cdot B^T = \begin{pmatrix}
         0.8 & 0.2
         \end{pmatrix} \times \begin{pmatrix}
         0.6 & 0.4
         \\ 0.3 & 0.7
         \end{pmatrix} = \begin{pmatrix}
         0.48+0.06 \\
         0.32+0.14
         \end{pmatrix} = \begin{pmatrix}
         0.54 \\
         0.46
         \end{pmatrix} = B^*$$
         
      2. 执行同样的过程，复用之前的结果，继续消除$B$获得$P(C,D)$。在算法的第3行中，通过$\varphi$调用函数SumProductEliminateVar的结果指派给$\varphi$，从而使用了之前步骤的结果：$$B^{*T} \cdot C^T = \begin{pmatrix}
         0.54 & 0.46
         \end{pmatrix} \times \begin{pmatrix}
         0.5 & 0.5
         \\ 0.8 & 0.2
         \end{pmatrix} = \begin{pmatrix}
         0.638 \\
         0.362
         \end{pmatrix} = C^*$$
         
      3. 现在只剩下两个变量$C$和$D$，同样调用结果消除$C$：$$C^{*T} \cdot D^T = \begin{pmatrix}
         0.638 & 0.362
         \end{pmatrix} \times \begin{pmatrix}
         0.3 & 0.7
         \\ 0.4 & 0.6
         \end{pmatrix} = \begin{pmatrix}
         0.3362 \\
         0.6638
         \end{pmatrix} = P(D)$$
         
    + 在R中可使用如下代码迅速得到结果：
      
      ```{r ABCD Sum-product, echo=TRUE, tidy=TRUE, warning=FALSE, message=FALSE, comment=""}
      Bs = t(A) %*% t(B)
      Bs
      Cs = Bs %*% t(C)
      Cs
      Ds = Cs %*% t(D)
      Ds
      ```
      
# 2.3 和积与信念更新
## 关于和积消除算法的3个问题

- 如果观察到一个变量，该如何计算其他变量子集的后验概率？

  * + 计算很简单：通过实例化$\varphi[E=e]$替换每个因子$\varphi$；如果采用上面的和积消除算法，若$Z$ 是查询子集，则可得到$P(Z,e)$；因此，根据贝叶斯公式，需进一步归一化处理，以获得所需的条件后验概率
  
    + 前面的算法可扩展成如下形式：$\alpha = \sum_{Z \in Val(Z)} \varphi^* (y)$，其中$\varphi^* = P(Z,e)$是前一步计算的边缘分布
    
    + 从而有$P(Y|e) = \frac {P(Y,e)} {P(e)} = \frac {\varphi^*} {\alpha}$

- 是否可能自动找出变量的最优（或者至少非常有效）序列？
  
  * + 使用联结树算法回答；它是当今概率图模型中最基础/最主要的算法
  
- 如果存在这样的序列，是否可以应用到任何类型的图中，特别是带有回路的图中？
  
  * + 使用联结树算法回答；它试图把任何类型的图形转换成具有变量簇（团）的树，从而可使用前面的算法，同时保证最优顺序和最小化的计算成本
  
# 2.4 联结树（团树）算法

- 算法基本思想：把概率图模型转换为一棵树，树的一些属性可确保后验概率的高效计算

- 算法不仅计算查询中的后验概率分布，还计算所有其他（未观察到的）变量的后验概率分布，可以得到任何变量的分布

- 以之前示例 P(ABCD)=P(A) \cdot P(B|A) \cdot P(C|B) \cdot P(D|C)解释算法思想：对一个因子使用贝叶斯规则，$$ P(ABCD)=P(A) \cdot  \frac {P(A,B)} {P(A)} \cdot \frac {P(B,C)}{P(B)} \cdot \frac {P(C,D)}{P(C)}=\frac {P(A,B) \cdot P(B,C) \cdot P(C,D)}{P(B) \cdot P(C)}$$

- 集合$\left \{A,B \right \}, \left \{B,C \right \}$和$\left \{B,C \right \}, \left \{C,D \right \}$交集中的变量作为分母，即$P(B)$和$P(D)$是上述集合的簇（团）交集（这并非通用，但这是从图模型构建树模型并执行推断的有用结果）

- 联结树的构建经过4个步骤，最终把图模型转换成团树

  * + 1. 对图模块化（节点排序）：使用无向边连接每个节点的父节点对（在具有共同子节点的成对节点之间添加一条弧（边）），如**图2-11**所示（与图2-10相同）
      
      ![图2-11](./图2-11.jpg "图2-11")
      
    + 2. 消掉边的方向：将有向图转换成一个无向图，即每个有向边（箭头）被无向边替代，使得每个变量（图中的节点）和父节点都在同一个团中，即所有节点都在相互连接的子图中
      
    + 3. 对图三角化：在无向图中添加额外的一条边使其三角化，如**图2-12**所示，虚线来源于三角化，而实线源自之前的两个步骤
      
      ![图2-12](./图2-12.jpg "图2-12")
      
    + 4. 构建一颗联结树：把三角化的图转换为一颗簇（团）树，其中每个节点表示变量子集中的因子；子集由图中的每个团确定，每个簇（团）节点之间有个分隔节点；团树算法如下：
    
      - 找出三角化图中的每个团，并给团节点加入新的节点
      
      - 计算图上的最大扩展树（团树即是）：团树的概率分布为$$P(\chi)=\frac {\prod _{c \in C} \varphi(c)}{\prod_{s \in S} \varphi(s)}$$
      
      - 其中$\varphi(c)$是团树中每个簇（团）的因子，$\varphi(s)$是团树中每个分隔的因子
      
      - 从来自《贝叶斯推理和机器学习》(Barber, Cambridge University Press, 2012)的示例中看一下完整转换的过程
      
        + 如**图2-13**所示初始图形
            
            ![图2-13](./图2-13.jpg "图2-13")
          
        + 对其三角化的无向图如**图2-14**所示
            
            ![图2-14](./图2-14.jpg "图2-14")
            
        + 最终联结/团树如**图2-15**所示
        
            ![图2-15](./图2-15.jpg "图2-15")
            
- 联结树上的推断是通过从一个簇（团）传递信息给另一个簇（团）实现的，传递路径有两种：自顶向下和自底向上
  
- 完成团之间的完整信息更新后，每个团都会包含自身变量的后验概率分布（例如顶层节点$P(ABC)$）；最后，找出任意变量的后验概率都归结为对其中一个团使用贝叶斯规则并边缘化不感兴趣的变量

# 2.4 联结树（团树）算法
## R程序包实现算法的练习

- 联结树算法的实现很复杂，但一些R程序包已完整实现；在第1章中，已有使用R程序包*gRain*进行贝叶斯推断的简单示例，其算法就是联结树算法；练习如**图2-16**的示例（与图2-12相同）

  ![图2-16](./图2-16.jpg "图2-16")
  
  * + 此图概率分布的分解如下：$$P(ABCDEF)=P(F) \cdot P(C|F) \cdot P(E|F) \cdot P(A|C) \cdot P(D|E) \cdot P(B|A,D)$$
  
    + 首先给R加载程序包*gRain*：
      ```{r grain, echo=TRUE, tidy=TRUE, warning=FALSE, message=FALSE, comment=""}
      library(gRain)
      ```
    
    + 然后创建从$A$到$F$的随机变量集合（即条件概率表），由此完全定义了概率图模型：
      ```{r cptable, echo=TRUE, tidy=TRUE, warning=FALSE, message=FALSE, comment=""}
      val= c("true", "false")
      F = cptable(~F, values=c(10,90),levels=val)
      C = cptable(~C|F, values=c(10,90,20,80),levels=val)
      E = cptable(~E|F, values=c(50,50,30,70),levels=val)
      A = cptable(~A|C, values=c(50,50,70,30),levels=val)
      D = cptable(~D|E, values=c(60,40,70,30),levels=val)
      B = cptable(~B|A:D, values=c(60,40,70,30,20,80,10,90),levels=val)
      ```
    
    + 接下来计算联结树，调用函数运行如下命令（算法会在一次运行中完成所有事情）：
      ```{r compileCPT, echo=TRUE, tidy=TRUE, warning=FALSE, message=FALSE, comment=""}
      plist = compileCPT(list(F,E,C,A,D,B))#按图结构进行节点排序
      plist
      ```
      
    + 可以查看一些变量的条件概率
      ```{r lookCPT, echo=TRUE, tidy=TRUE, warning=FALSE, message=FALSE, comment=""}
      print(plist$F)
      print(plist$B)
      ```
      
    + 最终创建了概率图模型，并通过如下命令调用联结树算法：
      ```{r creatjtree, echo=TRUE, tidy=TRUE, warning=FALSE, message=FALSE, comment=""}
      jtree = grain(plist)
      jtree
      ```
      
    + 有了联结树的图表示就可执行任何可能的推理；而且只需要计算联结树一次，所有查询都可使用同一联结树；当然，如果改变了联结树，则要重新计算；执行如下几个查询命令：
      ```{r executequery, echo=TRUE, tidy=TRUE, warning=FALSE, message=FALSE, comment=""}
      querygrain(jtree, nodes=c("F"), type="marginal")
      querygrain(jtree, nodes=c("C"), type="marginal")
      querygrain(jtree, nodes=c("B"), type="marginal")
      querygrain(jtree, nodes=c("A","B"), type="joint")
      querygrain(jtree, nodes=c("A","B","C"), type="joint")
      ```
      
    + 现在想观察变量并计算后验分布；假设$F=true$，则想把这个信息传播到图网络中的其余部分：
      ```{r creatjtree2, echo=TRUE, tidy=TRUE, warning=FALSE, message=FALSE, comment=""}
      jtree2 = setEvidence(jtree, evidence=list(F="true"))
      ```
      
    + 可以再次查询图网络：
      ```{r executequery2, echo=TRUE, tidy=TRUE, warning=FALSE, message=FALSE, comment=""}
      querygrain(jtree, nodes=c("F"), type="marginal")
      querygrain(jtree2, nodes=c("F"), type="marginal")
      querygrain(jtree, nodes=c("A"), type="marginal")
      querygrain(jtree2, nodes=c("A"), type="marginal")
      querygrain(jtree, nodes=c("B"), type="marginal")
      querygrain(jtree2, nodes=c("B"), type="marginal")
      ```
      
    + 设置更多证据（$F=true, A=false$）并在图网络中把它们进行前向和后向传播，也可计算逆概率：
      ```{r creatjtree3, echo=TRUE, tidy=TRUE, warning=FALSE, message=FALSE, comment=""}
      jtree3 = setEvidence(jtree, evidence=list(F="true",A="false"))
      ```
      
    + 可以再次查询图网络，看看设置证据前后结果的不同：
      ```{r executequery3, echo=TRUE, tidy=TRUE, warning=FALSE, message=FALSE, comment=""}
      querygrain(jtree, nodes=c("C"), type="marginal")
      querygrain(jtree2, nodes=c("C"), type="marginal")
      querygrain(jtree3, nodes=c("C"), type="marginal")
      ```
      
    + 正如期望的，知道$A$和$F$的值可以极大地改变$C$的概率分布；作为练习，可以设置$F$的证据（然后是$B$），看看$A$的后验概率的变化
    
# 2.5 概率图模型示例

- 本部分给出几个概率图模型的实例，它们都是理解精确推断的优秀示例

## 2.5.1 洒水器实例

- 假如我们在照看花园，草地是湿的；我们想知道草地为什么是湿的

- 有两种可能：之前下过雨或者我们忘记关掉洒水器；而且我们可观察天空/天气，如果是多云天气，就有可能之前下过雨；但是，如果是多云天气，我们很可能不会打开洒水器；因此在这个实例中，我们更有可能相信，我们并非忘记关掉洒水器；概率图模型如**图2-17所示**

  ![图2-17](./图2-17.jpg "图2-17")
  
  * + 这是一个因果推理的简单实例，确定4个随机变量：多云(cloudy)、洒水器(sprinkler)、下雨(rain)、草皮湿润(wetgrass)，都是二元变量
  
    + 可以给出父节点（变量）的先验分布：$P(cloudy=true)=P(cloudy=false)=0.5$
    
    + 对于其他节点（变量），可以设定条件概率表：
    
      |   $cloudy$  |   $P(rain=T|cloudy)$  | $P(rain=F|cloudy)$ |   
      |:-----------:|:---------------------:|:------------------:|
      |   $True$    |          0.8          |         0.2        |
      |   $False$   |          0.2          |         0.8        |

# 2.5 概率图模型示例
## 2.5.2 医疗专家系统

- 把症状和病因连接起来用于表示医疗知识；背后推理是证明病因可以导致可观测的症状；但问题是我们有很多症状，而且它们中有许多的原因都是相同的

- 用概率图模型表示医疗知识库的思想包括两层节点：一层是原因节点；一层是症状节点；如**图2-18**所示

  ![图2-18](./图2-18.jpg "图2-18")
  
  * + 每个节点的条件概率表都会强化或弱化症状和病因之间的连接，以便更好地表示每个症状最可能的原因
  
    + 依据关联的复杂程度，模型可能是优良的推理模型，也可能是欠佳的模型而不利于精确推断；而且大型概率表可能会是个问题（需确定太多的参数）；然而，使用真实数据库可以学习参数（下一章了解参数学习）
    
    + 在这个模型中，看到症状2和症状3有3个父节点（实际中可能更多）；例如，头痛症状可由很多不同原因引起
    
# 2.5 概率图模型示例
## 2.5.3 多于两层的模型

- 多于两层的模型在许多应用中更有意义，它拥有更深的因果推理过程，而且每个节点上都有相应的原因和结果，对于理解问题本身的结构也很自然

- J.Binder, D.Koller, S.Russell和K.Kanazawa的文章Adaptive Probabilistic
Networks with Hidden Variables. Machine Learning, 29(2-3):213-244, 1997，介绍了一个概率图模型用来估计一位汽车投保人的预期理赔费用

  ![图2-19](./图2-19.jpg "图2-19")
  
  * + 这类模型的复杂度并没有理论限制，但通常建议节点间的关系简单化，例如，每个节点拥有不超过3个父节点是比较好的策略
  
    + 在这个模型中，采用更多层的模型来表示关于汽车保险的知识；图中隐节点用阴影表示，输出节点用粗框表示
    
    + 有时模型可能会很复杂，但依然可用；例如，S.Andreassen, F.V.Jensen, S.K.Andersen, B.Falck, U.Kjærulff, M.Woldbye, A.R.Sørensen, A.Rosenfalck和F.Jensen的书MUNIN-an Expert EMG Assistant. In Computer-Aided Electromyography and Expert Systems (Elsevier
(Noth-Holland), 1989)第12章设计了一个复杂的图网络；可由R程序包bnlearn展示如**图2-20**

  ![图2-20](./图2-20.jpg "图2-20")
  
# 2.5 概率图模型示例
## 2.5.4 树结构的概率图模型

- 树结构的概率图模型通常可以生成非常高效的推理；模型中变量之间的关系很简单，每个节点都是只有一个父节点，但可以有很多子节点；模型中的任意变量可以表示成$P(X|Y)$的简单关系；如**图2-21**所示

  ![图2-21](./图2-21.jpg "图2-21")
  
  * + 在这个模型中，由联结树算法生成的节点簇（团）总是由两类节点构成：子节点和父节点；它可以保证联结树算法的复杂度较低，并支持快速推断
  
    + 其实所有的节点都可以连接在一起，但这样的模型会非常复杂；如果需要我们可以开发自己更加复杂的模型，但关键得理解节点之间的因果关系是什么样的
    
    + 可以基于真实数据挖掘学习图结构来设计新的模型，可从简单几个节点的模型开始，执行查询看看模型的表现如何，然后再扩展模型；后面我们将学到参数学习和结构学习，以及因果推断的相关内容
    
# 2.6 小结

- 推断的基础知识，以及计算后验概率的最重要算法

- 变量消除算法和联结树算法

- 学习考虑因果关系、时序关系以及确定变量之间的模式来构建图模型

- 接触图模型的一些基本特征，支持组合图形构建更加复杂的模型

- 学习R程序包中使用联结树算法执行推断，并利用同样的联结树用于边缘分布和联合分布任何类型的查询

- 学习几个真实世界的概率图模型实例，明确概率图模型通常是精确推断的优良备选方案

- 确定图中参数（条件概率表）很繁琐，我们将学习如何从数据集中自动找出参数；将会介绍EM (Expectation Maximization)算法，并尝试解决复杂问题：学习图本身的结构；推断是所有机器学习算法中最重要的子任务，因此很有必要设计诸如联结树这种高效算法


      








